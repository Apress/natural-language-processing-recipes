{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning for NLP"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 6-1. Retrieving Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np \n",
    "import nltk\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import scipy \n",
    "from scipy import spatial\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly taking sentences from internet \n",
    "\n",
    "Doc1 = [\"With the Union cabinet approving the amendments to the Motor Vehicles Act, 2016, those caught for drunken driving will have to have really deep pockets, as the fine payable in court has been enhanced to Rs 10,000 for first-time offenders.\" ] \n",
    "     \n",
    "Doc2 = [\"Natural language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\"]\n",
    "\n",
    "Doc3 = [\"He points out that public transport is very good in Mumbai and New Delhi, where there is a good network of suburban and metro rail systems.\"]\n",
    "\n",
    "Doc4 = [\"But the man behind the wickets at the other end was watching just as keenly. With an affirmative nod from Dhoni, India captain Rohit Sharma promptly asked for a review. Sure enough, the ball would have clipped the top of middle and leg.\"]\n",
    "\n",
    "# Put all the documents in one list\n",
    "\n",
    "fin= Doc1+Doc2+Doc3+Doc4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit\n",
    "\n",
    "#load the model\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "\n",
    "#Preprocessing \n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' \n",
    "    text = re.sub(pattern, '', ''.join(text))\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "# Function to get the embedding vector for n dimension, we have used \"300\"\n",
    "\n",
    "def get_embedding(word):\n",
    "    if word in model.wv.vocab:\n",
    "        return model[x]\n",
    "    else:\n",
    "        return np.zeros(300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting average vector for each document \n",
    "out_dict =  {}\n",
    "for sen in fin:\n",
    "    average_vector = (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(remove_stopwords(sen))]), axis=0))\n",
    "    dict = { sen : (average_vector) }\n",
    "    out_dict.update(dict)\n",
    "\n",
    "# Function to calculate the similarity between the query vector and document vector\n",
    "\n",
    "def get_sim(query_embedding, average_vector_doc):\n",
    "    sim = [(1 - scipy.spatial.distance.cosine(query_embedding, average_vector_doc))]\n",
    "    return sim\n",
    "\n",
    "# Rank all the documents based on the similarity to get relevant docs\n",
    "\n",
    "def Ranked_documents(query):\n",
    "    query_words =  (np.mean(np.array([get_embedding(x) for x in nltk.word_tokenize(query.lower())],dtype=float), axis=0))\n",
    "    rank = []\n",
    "    for k,v in out_dict.items():\n",
    "        rank.append((k, get_sim(query_words, v)))\n",
    "    rank = sorted(rank,key=lambda t: t[1], reverse=True)\n",
    "    print('Ranked Documents :')\n",
    "    return rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call the IR function with a query\n",
    "\n",
    "Ranked_documents(\"cricket\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Letâ€™s take one more example as may be driving. \n",
    "\n",
    "Ranked_documents(\"driving\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 6-2. Classifying Text with Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read file\n",
    "file_content = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "#check sample content in the email\n",
    "file_content['v2'][1]\n",
    "\n",
    "#Import library\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Remove stop words\n",
    "stop = stopwords.words('english')\n",
    "file_content['v2'] = file_content['v2'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# Delete unwanted columns\n",
    "Email_Data = file_content[['v1', 'v2']]\n",
    "\n",
    "# Rename column names\n",
    "Email_Data = Email_Data.rename(columns={\"v1\":\"Target\", \"v2\":\"Email\"})\n",
    "Email_Data.head()\n",
    "\n",
    "#Delete punctuations, convert text in lower case and delete the double space \n",
    "\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: re.sub('[!@#$:).;,?&]', '', x.lower()))\n",
    "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: re.sub(' ', ' ', x))\n",
    "Email_Data['Email'].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separating text(input) and target classes\n",
    "\n",
    "list_sentences_rawdata = Email_Data[\"Email\"].fillna(\"_na_\").values\n",
    "list_classes = [\"Target\"]\n",
    "target = Email_Data[list_classes].values\n",
    "\n",
    "\n",
    "To_Process=Email_Data[['Email', 'Target']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Train and test split with 80:20 ratio\n",
    "train, test = train_test_split(To_Process, test_size=0.2) \n",
    "\n",
    "# Define the sequence lengths, max number of words and embedding dimensions\n",
    "# Sequence length of each sentence. If more, truncate. If less, pad with zeros\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 300 \n",
    "\n",
    "# Top 20000 frequently occurring words\n",
    "MAX_NB_WORDS = 20000 \n",
    " \n",
    "# Get the frequently occurring words\n",
    " tokenizer = Tokenizer(num_words=MAX_NB_WORDS) \n",
    "tokenizer.fit_on_texts(train.Email) \n",
    "train_sequences = tokenizer.texts_to_sequences(train.Email)\n",
    "test_sequences = tokenizer.texts_to_sequences(test.Email)\n",
    "\n",
    "# dictionary containing words and their index\n",
    "word_index = tokenizer.word_index \n",
    "# print(tokenizer.word_index) \n",
    "# total words in the corpus\n",
    "print('Found %s unique tokens.' % len(word_index)) \n",
    "\n",
    "# get only the top frequent words on train\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH) \n",
    "\n",
    "# get only the top frequent words on test\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH) \n",
    "\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = train['Target']\n",
    "test_labels = test['Target']\n",
    "\n",
    "#import library\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# converts the character array to numeric array. Assigns levels to unique labels.\n",
    "\n",
    "le = LabelEncoder() \n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "test_labels = le.transform(test_labels)\n",
    "\n",
    "print(le.classes_)\n",
    "print(np.unique(train_labels, return_counts=True))\n",
    "print(np.unique(test_labels, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# changing data types\n",
    "labels_train = to_categorical(np.asarray(train_labels))\n",
    "labels_test = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', labels_train.shape)\n",
    "print('Shape of label tensor:', labels_test.shape)\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "print(MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Training CNN 1D model.')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS,\n",
    " EMBEDDING_DIM,\n",
    " input_length=MAX_SEQUENCE_LENGTH\n",
    " ))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    " optimizer='rmsprop',\n",
    " metrics=['acc'])\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    " batch_size=64,\n",
    " epochs=5,\n",
    " validation_data=(test_data, labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#predictions on test data\n",
    "\n",
    "predicted=model.predict(test_data)\n",
    "predicted\n",
    "\n",
    "#model evaluation\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "precision, recall, fscore, support = score(labels_test, predicted.round())\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "\n",
    "print(\"############################\")\n",
    "\n",
    "print(sklearn.metrics.classification_report(labels_test, predicted.round()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import library\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "\n",
    "#model training\n",
    "\n",
    "print('Training SIMPLERNN model.')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS,\n",
    " EMBEDDING_DIM,\n",
    " input_length=MAX_SEQUENCE_LENGTH\n",
    " ))\n",
    "model.add(SimpleRNN(2, input_shape=(None,1)))\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    " batch_size=16,\n",
    " epochs=5,\n",
    " validation_data=(test_data, labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction on test data\n",
    "predicted_Srnn=model.predict(test_data)\n",
    "predicted_Srnn\n",
    "\n",
    "#model evaluation\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "precision, recall, fscore, support = score(labels_test, predicted_Srnn.round())\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "\n",
    "print(\"############################\")\n",
    "\n",
    "print(sklearn.metrics.classification_report(labels_test, predicted_Srnn.round()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model training\n",
    "\n",
    "print('Training LSTM model.')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS,\n",
    " EMBEDDING_DIM,\n",
    " input_length=MAX_SEQUENCE_LENGTH\n",
    " ))\n",
    "model.add(LSTM(output_dim=16, activation='relu', inner_activation='hard_sigmoid',return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    " batch_size=16,\n",
    " epochs=5,\n",
    " validation_data=(test_data, labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prediction on text data\n",
    "predicted_lstm=model.predict(test_data)\n",
    "predicted_lstm\n",
    "\n",
    "#model evaluation \n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "precision, recall, fscore, support = score(labels_test, predicted_lstm.round())\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "\n",
    "print(\"############################\")\n",
    "\n",
    "print(sklearn.metrics.classification_report(labels_test, predicted_lstm.round()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model training\n",
    "\n",
    "print('Training Bidirectional LSTM model.')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS,\n",
    " EMBEDDING_DIM,\n",
    " input_length=MAX_SEQUENCE_LENGTH\n",
    " ))\n",
    "model.add(Bidirectional(LSTM(16, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\n",
    "model.add(Conv1D(16, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\"))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(50, activation=\"relu\"))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "\n",
    "model.fit(train_data, labels_train,\n",
    " batch_size=16,\n",
    " epochs=3,\n",
    " validation_data=(test_data, labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction on test data\n",
    "\n",
    "predicted_blstm=model.predict(test_data)\n",
    "predicted_blstm\n",
    "\n",
    "#model evaluation\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "precision, recall, fscore, support = score(labels_test, predicted_blstm.round())\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "\n",
    "print(\"############################\")\n",
    "\n",
    "print(sklearn.metrics.classification_report(labels_test, predicted_blstm.round()))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 6-3. Next word/sequence of words suggestion â€“ Next word prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_content = pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "# Just selecting emails and connverting it into list\n",
    "Email_Data = file_content[[ 'v2']]\n",
    "\n",
    "list_data = Email_Data.values.tolist()\n",
    "list_data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import codecs\n",
    "import collections\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import scipy \n",
    "from scipy import spatial\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "tokenizer = ToktokTokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Converting list to string\n",
    "from collections import Iterable\n",
    "\n",
    "\n",
    "def flatten(items):\n",
    "    \"\"\"Yield items from any nested iterable\"\"\"\n",
    "    for x in items:\n",
    "        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n",
    "            for sub_x in flatten(x):\n",
    "                yield sub_x\n",
    "        else:\n",
    "            yield x\n",
    "\n",
    "\n",
    "TextData=list(flatten(list_data))  \n",
    "TextData = ''.join(TextData) \n",
    "\n",
    "# Remove unwanted lines and converting into lower case\n",
    "TextData = TextData.replace('\\n','')\n",
    "TextData = TextData.lower() \n",
    "\n",
    "pattern = r'[^a-zA-z0-9\\s]' \n",
    "TextData = re.sub(pattern, '', ''.join(TextData)) \n",
    "\n",
    "# Tokenizing\n",
    "\n",
    "tokens = tokenizer.tokenize(TextData)\n",
    "tokens = [token.strip() for token in tokens] \n",
    "\n",
    "# get the distinct words and sort it\n",
    "\n",
    "word_counts = collections.Counter(tokens)\n",
    "word_c = len(word_counts)\n",
    "print(word_c)\n",
    "\n",
    "distinct_words = [x[0] for x in word_counts.most_common()]\n",
    "distinct_words_sorted = list(sorted(distinct_words)) \n",
    "\n",
    "\n",
    "# Generate indexing for all words\n",
    "\n",
    "word_index = {x: i for i, x in enumerate(distinct_words_sorted)} \n",
    "\n",
    "\n",
    "# decide on sentence lenght\n",
    "\n",
    "sentence_length = 25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prepare the dataset of input to output pairs encoded as integers\n",
    "# Generate the data for the model\n",
    "\n",
    "#input = the input sentence to the model with index \n",
    "#output = output of the model with index\n",
    "\n",
    "InputData = []\n",
    "OutputData = []\n",
    "\n",
    "for i in range(0, word_c - sentence_length, 1):\n",
    "    X = tokens[i:i + sentence_length]\n",
    "    Y = tokens[i + sentence_length]\n",
    "    InputData.append([word_index[char] for char in X])\n",
    "    OutputData.append(word_index[Y])\n",
    "\n",
    "print (InputData[:1])\n",
    "print (\"\\n\")\n",
    "print(OutputData[:1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate  X \n",
    "X = numpy.reshape(InputData, (len(InputData), sentence_length, 1))\n",
    "\n",
    "\n",
    "# One hot encode the output variable\n",
    "Y = np_utils.to_categorical(OutputData) \n",
    "\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    " \n",
    "#define the checkpoint\n",
    "file_name_path=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_name_path, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks = [checkpoint] \n",
    "\n",
    "#fit the model\n",
    "model.fit(X, Y, epochs=5, batch_size=128, callbacks=callbacks) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "file_name = \"weights-improvement-05-6.8213.hdf5\"\n",
    "model.load_weights(file_name)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating random sequence\n",
    "start = numpy.random.randint(0, len(InputData))\n",
    "input_sent = InputData[start]\n",
    "\n",
    "# Generate index of the next word of the email \n",
    "\n",
    "X = numpy.reshape(input_sent, (1, len(input_sent), 1))\n",
    "predict_word = model.predict(X, verbose=0)\n",
    "index = numpy.argmax(predict_word)\n",
    "\n",
    "print(input_sent)\n",
    "print (\"\\n\")\n",
    "print(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert these indexes back to words\n",
    "\n",
    "word_index_rev = dict((i, c) for i, c in enumerate(tokens))\n",
    "result = word_index_rev[index]\n",
    "sent_in = [word_index_rev[value] for value in input_sent]\n",
    "\n",
    "print(sent_in)\n",
    "print (\"\\n\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
