{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Industry Applications"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 5-1. Implementing Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from io import StringIO\n",
    "import seaborn as sns\n",
    "\n",
    "#Importing the data which was downloaded in the last step\n",
    "\n",
    "Data = pd.read_csv(\"/Consumer_Complaints.csv\",encoding='latin-1')\n",
    "\n",
    "#Understanding the columns\n",
    "\n",
    "Data.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Selecting required columns and rows\n",
    "Data = Data[['product', 'consumer_complaint_narrative']]\n",
    "Data = Data[pd.notnull(Data['consumer_complaint_narrative'])]\n",
    "\n",
    "# See top 5 rows\n",
    "Data.head()\n",
    "\n",
    "# Factorizing the category column\n",
    "Data['category_id'] = Data['product'].factorize()[0]\n",
    "\n",
    "Data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check the distriution of complaints by category\n",
    "Data.groupby('product').consumer_complaint_narrative.count()\n",
    "\n",
    "\n",
    "# Lets plot it and see\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "Data.groupby('product').consumer_complaint_narrative.count().plot.bar(ylim=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into train and validation\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(Data['consumer_complaint_narrative'], Data['product'])\n",
    "\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "\n",
    "tfidf_vect.fit(Data['consumer_complaint_narrative'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = linear_model.LogisticRegression().fit(xtrain_tfidf, train_y)\n",
    "\n",
    "\n",
    "# Model summary\n",
    "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
    "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
    "          verbose=0, warm_start=False)\n",
    "\n",
    "# Checking accuracy\n",
    "\n",
    "accuracy = metrics.accuracy_score(model.predict(xvalid_tfidf), valid_y)\n",
    "print (\"Accuracy: \", accuracy)\n",
    "\n",
    "# Classification report\n",
    "\n",
    "print(metrics.classification_report(valid_y, model.predict(xvalid_tfidf),target_names=Data['product'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "\n",
    "conf_mat = confusion_matrix(valid_y, model.predict(xvalid_tfidf))\n",
    "\n",
    "# Vizualizing confusion matrix\n",
    "\n",
    "category_id_df = Data[['product', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'product']].values)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap=\"BuPu\",\n",
    "            xticklabels=category_id_df[['product']].values, yticklabels=category_id_df[['product']].values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction example\n",
    "\n",
    "texts = [\"This company refuses to provide me verification and validation of debt\"+ \n",
    "         \"per my right under the FDCPA. I do not believe this debt is mine.\"]\n",
    "text_features = tfidf_vect.transform(texts)\n",
    "predictions = model.predict(text_features)\n",
    "print(texts)\n",
    "print(\"  - Predicted as: '{}'\".format(id_to_category[predictions[0]]))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 5-2. Implementing Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "#Read the data\n",
    "df = pd.read_csv('Reviews.csv')\n",
    "\n",
    "# Look at the top 5 rows of the data\n",
    "df.head(5)\n",
    "\n",
    "# Understand the data types of the columns\n",
    "df.info()\n",
    "\n",
    "# Looking at the summary of the reviews.\n",
    "df.Summary.head(5)\n",
    "\n",
    "# Looking at the description of the reviews\n",
    "df.Text.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "\n",
    "# Lower casing and removing punctuations\n",
    "df['Text'] = df['Text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df['Text'] = df['Text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "# Removal of stop words\n",
    "stop = stopwords.words('english')\n",
    "df['Text'] = df['Text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "\n",
    "# Spelling correction\n",
    "df['Text'] = df['Text'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "# Lemmatization\n",
    "df['Text'] = df['Text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "df.Text.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new data frame “reviews” to perform exploratory data analysis upon that\n",
    "\n",
    "reviews = df\n",
    "\n",
    "# Dropping null values\n",
    "reviews.dropna(inplace=True) \n",
    "\n",
    "# The histogram reveals this dataset is highly unbalanced towards high rating. \n",
    "\n",
    "reviews.Score.hist(bins=5,grid=False)\n",
    "plt.show()\n",
    "print(reviews.groupby('Score').count().Id)\n",
    "\n",
    "# To make it balanced data, we sampled each score by the lowest n-count from above. (i.e. 29743 reviews scored as '2')\n",
    "\n",
    "score_1 = reviews[reviews['Score'] == 1].sample(n=29743)\n",
    "score_2 = reviews[reviews['Score'] == 2].sample(n=29743)\n",
    "score_3 = reviews[reviews['Score'] == 3].sample(n=29743)\n",
    "score_4 = reviews[reviews['Score'] == 4].sample(n=29743)\n",
    "score_5 = reviews[reviews['Score'] == 5].sample(n=29743)\n",
    "\n",
    "# Here we recreate a 'balanced' dataset.\n",
    "reviews_sample = pd.concat([score_1,score_2,score_3,score_4,score_5],axis=0)\n",
    "reviews_sample.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Printing count by 'Score' to check dataset is now balanced.\n",
    "print(reviews_sample.groupby('Score').count().Id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's build a word cloud looking at the 'Summary'  text\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "# Wordcloud function's input needs to be a single string of text.\n",
    "# Here I'm concatenating all Summaries into a single string.\n",
    "# similarly you can build for Text column\n",
    "\n",
    "reviews_str = reviews_sample.Summary.str.cat()\n",
    "\n",
    "wordcloud = WordCloud(background_color='white').generate(reviews_str)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud,interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's split the data into Negative (Score is 1 or 2) and Positive (4 or #5) Reviews.\n",
    "negative_reviews = reviews_sample[reviews_sample['Score'].isin([1,2]) ]\n",
    "positive_reviews = reviews_sample[reviews_sample['Score'].isin([4,5]) ]\n",
    "\n",
    "# Transform to single string\n",
    "negative_reviews_str = negative_reviews.Summary.str.cat()\n",
    "positive_reviews_str = positive_reviews.Summary.str.cat()\n",
    "\n",
    "# Create wordclouds\n",
    "wordcloud_negative = WordCloud(background_color='white').generate(negative_reviews_str)\n",
    "wordcloud_positive = WordCloud(background_color='white').generate(positive_reviews_str)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax1 = fig.add_subplot(211)\n",
    "ax1.imshow(wordcloud_negative,interpolation='bilinear')\n",
    "ax1.axis(\"off\")\n",
    "ax1.set_title('Reviews with Negative Scores',fontsize=20)\n",
    "ax2 = fig.add_subplot(212)\n",
    "ax2.imshow(wordcloud_positive,interpolation='bilinear')\n",
    "ax2.axis(\"off\")\n",
    "ax2.set_title('Reviews with Positive Scores',fontsize=20)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "plt.style.use('fivethirtyeight')\n",
    "# Function for getting the sentiment\n",
    "cp = sns.color_palette()\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Generating sentiment for all the sentence present in the dataset\n",
    "emptyline=[]\n",
    "for row in df['Text']:\n",
    "    vs=analyzer.polarity_scores(row)\n",
    "    emptyline.append(vs)\n",
    "\n",
    "# Creating new dataframe with sentiments    \n",
    "df_sentiments=pd.DataFrame(emptyline)\n",
    "df_sentiments.head(5)\n",
    "\n",
    "# Merging the sentiments back to reviews dataframe\n",
    "df_c = pd.concat([df.reset_index(drop=True), d], axis=1)\n",
    "df_c.head(3)\n",
    "\n",
    "# Convert scores into positive and negetive sentiments using some threshold\n",
    "df_c['Sentiment'] = np.where(df_c['compound'] >= 0 , 'Positive', 'Negative')\n",
    "df_c.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result=df_c['Sentiment'].value_counts()\n",
    "result.plot(kind='bar', rot=0,color='br');\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 5-3. Applying text similarity functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import package\n",
    "!pip install recordlinkage\n",
    "import recordlinkage\n",
    "\n",
    "#For this demo let us use the inbuilt dataset from recordlinkage library\n",
    "\n",
    "#import data set\n",
    "from recordlinkage.datasets import load_febrl1 \n",
    "\n",
    "#create a dataframe - dfa\n",
    "dfA = load_febrl1()\n",
    "dfA.head()\n",
    "\n",
    "indexer = recordlinkage.BlockIndex(on='given_name')\n",
    "pairs = indexer.index(dfA)\n",
    "\n",
    "print (len(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell can take some time to compute.\n",
    "compare_cl = recordlinkage.Compare()\n",
    "\n",
    "compare_cl.string('given_name', 'given_name',method='jarowinkler', label='given_name')\n",
    "compare_cl.string('surname', 'surname', method='jarowinkler', label='surname')\n",
    "compare_cl.exact('date_of_birth', 'date_of_birth', label='date_of_birth')\n",
    "compare_cl.exact('suburb', 'suburb', label='suburb')\n",
    "compare_cl.exact('state', 'state', label='state')\n",
    "compare_cl.string('address_1', 'address_1',method='jarowinkler', label='address_1')\n",
    "\n",
    "features = compare_cl.compute(pairs, dfA)\n",
    "features.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select all the features except for given_name since its our blocking key\n",
    "features1 = features[['suburb','state','surname','date_of_birth','address_1']]\n",
    "\n",
    "# Unsupervised learning – probabilistic\n",
    "\n",
    "ecm = recordlinkage.ECMClassifier()\n",
    "result_ecm = ecm.learn((features1).astype(int),return_type = 'series')\n",
    "\n",
    "result_ecm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let us use the inbuilt dataset from recordlinkage library\n",
    "\n",
    "from recordlinkage.datasets import load_febrl4\n",
    "\n",
    "dfA, dfB = load_febrl4()\n",
    "dfA.head()\n",
    "\n",
    "# Same as explained previously, considering given_name as blocking index\n",
    "\n",
    "indexer = recordlinkage.BlockIndex(on='given_name')\n",
    "pairs = indexer.index(dfA, dfB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Explanation remains same\n",
    "compare_cl = recordlinkage.Compare()\n",
    "\n",
    "compare_cl.string('given_name', 'given_name',method='jarowinkler', label='given_name')\n",
    "compare_cl.string('surname', 'surname', method='jarowinkler', label='surname')\n",
    "compare_cl.exact('date_of_birth', 'date_of_birth', label='date_of_birth')\n",
    "compare_cl.exact('suburb', 'suburb', label='suburb')\n",
    "compare_cl.exact('state', 'state', label='state')\n",
    "compare_cl.string('address_1', 'address_1',method='jarowinkler', label='address_1')\n",
    "\n",
    "features = compare_cl.compute(pairs, dfA, dfB)\n",
    "\n",
    "features.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select all the features except for given_name since its our blocking key\n",
    "features1 = features[['suburb','state','surname','date_of_birth','address_1']]\n",
    "\n",
    "# unsupervised learning - probablistic\n",
    "ecm = recordlinkage.ECMClassifier()\n",
    "result_ecm = ecm.learn((features1).astype(int),return_type = 'series')\n",
    "\n",
    "result_ecm\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 5-4. Summarizing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import BeautifulSoup and urllib libraries to fetch data from Wikipedia.\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "# Function to get data from Wikipedia\n",
    "\n",
    "def get_only_text(url):\n",
    " page = urlopen(url)\n",
    " soup = BeautifulSoup(page)\n",
    " text = ' '.join(map(lambda p: p.text, soup.find_all('p')))\n",
    " print (text) \n",
    " return soup.title.text, text  \n",
    "\n",
    "# Mention the Wikipedia url\n",
    "url=”https://en.wikipedia.org/wiki/Natural_language_processing”\n",
    "\n",
    "# Call the function created above\n",
    "text = get_only_text(url) \n",
    "\n",
    "# Count the number of letters\n",
    "len(''.join(text))\n",
    "# Lets see first 1000 letters from the text\n",
    "text[:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import summarize from gensim\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.summarization import keywords\n",
    "\n",
    "# Convert text to string format\n",
    "text = str(text)\n",
    "\n",
    "#Summarize the text with ratio 0.1 (10% of the total words.)\n",
    "summarize(text, ratio=0.1)\n",
    "\n",
    "#keywords\n",
    "print(keywords(text, ratio=0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install sumy\n",
    "\n",
    "!pip install sumy\n",
    "\n",
    "# Import the packages\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "from sumy.summarizers.luhn import LuhnSummarizer \n",
    "\n",
    "# Extracting and summarizing\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 10\n",
    "    \n",
    "url=\"https://en.wikipedia.org/wiki/Natural_language_processing\"\n",
    "   \n",
    "parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))\n",
    "summarizer = LsaSummarizer()\n",
    "summarizer = LsaSummarizer(Stemmer(LANGUAGE))\n",
    "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 5-5. Clustering Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install mpld3\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "\n",
    "#Lets use the same complaint dataset we use for classification\n",
    "Data = pd.read_csv(\"/Consumer_Complaints.csv\",encoding='latin-1')\n",
    "\n",
    "#selecting required columns and rows\n",
    "Data = Data[['consumer_complaint_narrative']]\n",
    "Data = Data[pd.notnull(Data['consumer_complaint_narrative'])]\n",
    "\n",
    "# lets do the clustering for just 200 documents. Its easier to interpret.\n",
    "Data_sample=Data.sample(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove unwanted symbol\n",
    "\n",
    "Data_sample['consumer_complaint_narrative'] = Data_sample['consumer_complaint_narrative'].str.replace('XXXX','')\n",
    "\n",
    "# Convert dataframe to list\n",
    "complaints = Data_sample['consumer_complaint_narrative'].tolist()\n",
    "\n",
    "# create the rank of documents – we will use it later\n",
    "\n",
    "ranks = []\n",
    "for i in range(1, len(complaints)+1):\n",
    "    ranks.append(i)\n",
    "\n",
    "# Stop Words\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Load 'stemmer'\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Functions for sentence tokenizer, to remove numeric tokens and raw #punctuation\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# tfidf vectorizer \n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
    "                                 min_df=0.2, stop_words='english',\n",
    "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "\n",
    "#fit the vectorizer to data\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(complaints) \n",
    "terms = tfidf_vectorizer.get_feature_names()\n",
    "print(tfidf_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import Kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define number of clusters\n",
    "num_clusters = 6\n",
    "\n",
    "#Running clustering algorithm\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "#final clusters\n",
    "clusters = km.labels_.tolist()\n",
    "complaints_data = { 'rank': ranks, 'complaints': complaints, 'cluster': clusters }\n",
    "frame = pd.DataFrame(complaints_data, index = [clusters] , columns = ['rank', 'cluster'])\n",
    "\n",
    "#number of docs per cluster \n",
    "frame['cluster'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "for i in complaints:\n",
    "    allwords_stemmed = tokenize_and_stem(i) \n",
    "    totalvocab_stemmed.extend(allwords_stemmed) \n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "\n",
    "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :6]: \n",
    "        print(' %s' % vocab_frame.ix[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Similarity\n",
    "\n",
    "similarity_distance = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "\n",
    "# Convert two components as we're plotting points in a two-dimensional plane\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(similarity_distance)  # shape (n_components, n_samples)\n",
    "xs, ys = pos[:, 0], pos[:, 1]\n",
    "\n",
    "#Set up colors per clusters using a dict\n",
    "cluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e',5: '#D2691E'}\n",
    "\n",
    "#set up cluster names using a dict\n",
    "cluster_names = {0: 'property, based, assist', \n",
    "                 1: 'business, card', \n",
    "                 2: 'authorized, approved, believe', \n",
    "                 3: 'agreement, application,business', \n",
    "                 4: 'closed, applied, additional',\n",
    "                 5: 'applied, card'}\n",
    "\n",
    "# Finally plot it\n",
    "%matplotlib inline \n",
    "\n",
    "#Create data frame that has the result of the MDS and the cluster \n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters)) \n",
    "groups = df.groupby('label')\n",
    "\n",
    "# Set up plot\n",
    "fig, ax = plt.subplots(figsize=(17, 9)) # set size\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=20, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          \n",
    "        which='both',      \n",
    "        bottom='off',     \n",
    "        top='off',         \n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',        \n",
    "        which='both',    \n",
    "        left='off',     \n",
    "        top='off',       \n",
    "        labelleft='off')\n",
    "    \n",
    "ax.legend(numpoints=1) \n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
