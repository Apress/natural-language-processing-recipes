{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text to features"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-1. Converting Text to Features Using One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing the library \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Generating the features\n",
    "\n",
    "pd.get_dummies(Text.split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-2. Converting Text to Features Using Count Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing the function \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Text\n",
    "\n",
    "text = [\"I love NLP and I will learn NLP in 2month \"]\n",
    "\n",
    "# create the transform\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenizing\n",
    "\n",
    "vectorizer.fit(text)\n",
    "\n",
    "\n",
    "# encode document\n",
    "\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# summarize & generating output\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vector.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-3. Generating N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Text = \"I am learning NLP\"\n",
    "\n",
    "#Import textblob\n",
    "from textblob import TextBlob\n",
    "\n",
    "#For unigram : Use n = 1\n",
    "\n",
    "TextBlob(Text).ngrams(1)\n",
    "\n",
    "#For Bigram : For bigrams, use n = 2 \n",
    "\n",
    "TextBlob(Text).ngrams(2)\n",
    "\n",
    "#importing the function \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Text\n",
    "\n",
    "text = [\"I love NLP and I will learn NLP in 2month \"]\n",
    "\n",
    "# create the transform\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "# tokenizing\n",
    "\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# encode document\n",
    "\n",
    "\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# summarize & generating output\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import nltk\n",
    "from nltk import bigrams    \n",
    "import itertools\n",
    "\n",
    "def co_occurrence_matrix(corpus):\n",
    "    vocab = set(corpus)\n",
    "    vocab = list(vocab)\n",
    "    vocab_to_index = { word:i for i, word in enumerate(vocab) }\n",
    "    # Create bigrams from all words in corpus\n",
    "    bi_grams = list(bigrams(corpus))\n",
    "    # Frequency distribution of bigrams ((word1, word2), num_occurrences)\n",
    "    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n",
    "    # Initialise co-occurrence matrix\n",
    "    # co_occurrence_matrix[current][previous]\n",
    "    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    # Loop through the bigrams taking the current and previous word,\n",
    "    # and the number of occurrences of the bigram.\n",
    "    for bigram in bigram_freq:\n",
    "        current = bigram[0][1]\n",
    "        previous = bigram[0][0]\n",
    "        count = bigram[1]\n",
    "        pos_current = vocab_to_index[current]\n",
    "        pos_previous = vocab_to_index[previous]\n",
    "        co_occurrence_matrix[pos_current][pos_previous] = count \n",
    "    co_occurrence_matrix = np.matrix(co_occurrence_matrix)\n",
    "    # return the matrix and the index\n",
    "    return co_occurrence_matrix,vocab_to_index\n",
    "\n",
    "# sentences for testing\n",
    "\n",
    "sentences = [['I', 'love', 'nlp'],\n",
    "\t\t\t['I', 'love','to' 'learn'],\n",
    "\t\t\t['nlp', 'is', 'future'],\n",
    "\t\t\t['nlp', 'is', 'cool']]\n",
    "\n",
    "# create one list using many lists\n",
    "\n",
    "merged = list(itertools.chain.from_iterable(sentences))\n",
    "matrix = co_occurrence_matrix(merged)\n",
    "\n",
    "# generate the matrix\n",
    "\n",
    "CoMatrixFinal = pd.DataFrame(matrix[0], index=vocab_to_index, columns=vocab_to_index)\n",
    "print(CoMatrixFinal)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "\n",
    "# Transform\n",
    "vectorizer = HashingVectorizer(n_features=10)\n",
    "\n",
    "# create the hashing vector\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# summarize the vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray()) \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-6. Converting Text to Features Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quick': 6, 'jumped': 3, 'the': 7, 'over': 5, 'brown': 0, 'fox': 2, 'dog': 1, 'lazy': 4}\n",
      "[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n"
     ]
    }
   ],
   "source": [
    "Text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\"The dog.\",\n",
    "\"The fox\"]\n",
    "\n",
    "#Import TfidfVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Create the transform\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "#Tokenize and build vocab\n",
    "\n",
    "vectorizer.fit(Text)\n",
    "\n",
    "#Summarize\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-7. Implementing Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [['I', 'love', 'nlp'],\n",
    "\t\t\t['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
    "\t\t\t['nlp', 'is', 'future'],\n",
    "\t\t\t['nlp', 'saves', 'time', 'and', 'solves', 'lot', 'of', 'industry', 'problems'],\n",
    "\t\t\t['nlp', 'uses', 'machine', 'learning']]\n",
    "\n",
    "\n",
    "#import library \n",
    "\n",
    "!pip install gensim\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# training the model\n",
    "\n",
    "skipgram = Word2Vec(sentences, size =50, window = 3, min_count=1,sg = 1)\n",
    "print(skipgram)\n",
    "\n",
    "# access vector for one word\n",
    "\n",
    "print(skipgram['nlp'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# access vector for another one word\n",
    "\n",
    "print(fast['deep'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model\n",
    "\n",
    "skipgram.save('skipgram.bin')\n",
    "\n",
    "# load model\n",
    "\n",
    "skipgram = Word2Vec.load('skipgram.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# T â€“ SNE plot \n",
    "\n",
    "X = skipgram[skipgram.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(skipgram.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Continuous Bag of Words (CBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import library \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#Example sentences\n",
    "\n",
    "sentences = [['I', 'love', 'nlp'],\n",
    "\t\t\t['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
    "\t\t\t['nlp', 'is', 'future'],\n",
    "\t\t\t['nlp', 'saves', 'time', 'and', 'solves', 'lot', 'of', 'industry', 'problems'],\n",
    "\t\t\t['nlp', 'uses', 'machine', 'learning']]\n",
    "\n",
    "\n",
    "# training the model\n",
    "\n",
    "skipgram = Word2Vec(sentences, size =50, window = 3, min_count=1,sg = 1)\n",
    "print(skipgram)\n",
    "\n",
    "# access vector for one word\n",
    "\n",
    "print(skipgram['nlp'])\n",
    "\n",
    "# save model\n",
    "\n",
    "skipgram.save('skipgram.bin')\n",
    "\n",
    "# load model\n",
    "\n",
    "skipgram = Word2Vec.load('skipgram.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# T â€“ SNE plot \n",
    "\n",
    "X = skipgram[skipgram.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(skipgram.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import gensim package\n",
    "\n",
    "import gensim\n",
    "\n",
    "# load the saved model \n",
    " \n",
    "model = gensim.models.Word2Vec.load_word2vec_format('C:\\\\Users\\\\GoogleNews-vectors-negative300.bin', binary=True)  \n",
    " \n",
    "\n",
    "#Checking how similarity works. \n",
    "\n",
    "print (model.similarity('this', 'is'))\n",
    "\n",
    "#Lets check one more.\n",
    "print (model.similarity('post', 'book'))\n",
    "\n",
    "# Finding the odd one out.\n",
    "\n",
    "model.doesnt_match('breakfast cereal dinner lunch';.split())\n",
    "\n",
    "# It is also finding the relations between words. \n",
    "\n",
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 3-8 Implementing FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import FastText\n",
    "\n",
    "from gensim.models import FastText\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#Example sentences\n",
    "\n",
    "sentences = [['I', 'love', 'nlp'],\n",
    "\t\t\t['I', 'will', 'learn', 'nlp', 'in', '2','months'],\n",
    "\t\t\t['nlp', 'is', 'future'],\n",
    "\t\t\t['nlp', 'saves', 'time', 'and', 'solves', 'lot', 'of', 'industry', 'problems'],\n",
    "\t\t\t['nlp', 'uses', 'machine', 'learning']]\n",
    "\n",
    "\n",
    "fast = FastText(sentences,size=20, window=1, min_count=1, workers=5, min_n=1, max_n=2)\n",
    "\n",
    "# vector for word nlp\n",
    "\n",
    "print(fast['nlp'])\n",
    "print(fast['deep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "\n",
    "fast = Word2Vec.load('fast.bin')\n",
    "\n",
    "# visualize \n",
    "\n",
    "X = skipgram[skipgram.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "# create a scatter plot of the projection\n",
    "\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(skipgram.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
