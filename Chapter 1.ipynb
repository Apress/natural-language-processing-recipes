{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 1-1. Collecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tweepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-16918fd2e188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'tweepy'"
     ]
    }
   ],
   "source": [
    "# Install tweepy\n",
    "!pip install tweepy\n",
    "\n",
    "# Import the libraries\n",
    "\n",
    "import numpy as np\n",
    "import tweepy\n",
    "import json\n",
    "import pandas as pd\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "# credentials\n",
    "\n",
    "consumer_key = \"adjbiejfaaoeh\"\n",
    "consumer_secret = \"had73haf78af\"\n",
    "access_token = \"jnsfby5u4yuawhafjeh\"\n",
    "access_token_secret = \"jhdfgay768476r\"\n",
    "\n",
    "# calling API\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Provide the query you want to pull the data. For example, pulling data for the mobile phone ABC\n",
    "\n",
    "query =\"ABC\"\n",
    "\n",
    "# Fetching tweets\n",
    "\n",
    "Tweets = api.search(query, count = 10,lang='en',exclude='retweets',tweet_mode='extended')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 1-2. Collecting Data from PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install PyPDF2\n",
    "import PyPDF2\n",
    "from PyPDF2 import PdfFileReader\n",
    "\n",
    "#Creating a pdf file object\n",
    "\n",
    "pdf = open(\"file.pdf\",\"rb\")\n",
    "\n",
    "#creating pdf reader object\n",
    "\n",
    "pdf_reader = PyPDF2.PdfFileReader(pdf)\n",
    "\n",
    "#checking number of pages in a pdf file\n",
    "\n",
    "print(pdf_reader.numPages)\n",
    "\n",
    "#creating a page object\n",
    "\n",
    "page = pdf_reader.getPage(0)\n",
    "\n",
    "#finally extracting text from the page\n",
    "\n",
    "print(page.extractText())\n",
    "\n",
    "#closing the pdf file\n",
    "\n",
    "pdf.close()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 1-3.Collecting Data from Word Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Install docx \n",
    "!pip install docx\n",
    "\n",
    "#Import library\n",
    "from docx import Document\n",
    "\n",
    "#Creating a word file object\n",
    "\n",
    "doc = open(\"file.docx\",\"rb\")\n",
    "\n",
    "#creating word reader object\n",
    "\n",
    "document = docx.Document(doc)\n",
    "\n",
    "# create an empty string and call this document. This document variable store each paragraph in the Word document.We then create a for loop that goes through each paragraph in the Word document and appends the paragraph.\n",
    "\n",
    "docu=\"\"\n",
    "for para in document.paragraphs:\n",
    "    docu += para.text \n",
    "\n",
    "#to see the output call docu\n",
    "print(docu)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 1-4.Collecting Data from JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "#json from \"https://quotes.rest/qod.json\"\n",
    "r = requests.get(\"https://quotes.rest/qod.json\")\n",
    "res = r.json()\n",
    "print(json.dumps(res, indent = 4)) \n",
    "\n",
    "#output\n",
    "{\n",
    "    \"success\": {\n",
    "        \"total\": 1\n",
    "    },\n",
    "    \"contents\": {\n",
    "        \"quotes\": [\n",
    "            {\n",
    "                \"quote\": \"Where there is ruin, there is hope for a treasure.\",\n",
    "                \"length\": \"50\",\n",
    "                \"author\": \"Rumi\",\n",
    "                \"tags\": [\n",
    "                    \"failure\",\n",
    "                    \"inspire\",\n",
    "                    \"learning-from-failure\"\n",
    "                ],\n",
    "                \"category\": \"inspire\",\n",
    "                \"date\": \"2018-09-29\",\n",
    "                \"permalink\": \"https://theysaidso.com/quote/dPKsui4sQnQqgMnXHLKtfweF/rumi-where-there-is-ruin-there-is-hope-for-a-treasure\",\n",
    "                \"title\": \"Inspiring Quote of the day\",\n",
    "                \"background\": \"https://theysaidso.com/img/bgs/man_on_the_mountain.jpg\",\n",
    "                \"id\": \"dPKsui4sQnQqgMnXHLKtfweF\"\n",
    "            }\n",
    "        ],\n",
    "        \"copyright\": \"2017-19 theysaidso.com\"\n",
    "    }\n",
    "}\n",
    "\n",
    "#extract contents\n",
    "q = res['contents']['quotes'][0] \n",
    "q\n",
    "\n",
    "#output\n",
    "\n",
    "{'author': 'Rumi',\n",
    " 'background': 'https://theysaidso.com/img/bgs/man_on_the_mountain.jpg',\n",
    " 'category': 'inspire',\n",
    " 'date': '2018-09-29',\n",
    " 'id': 'dPKsui4sQnQqgMnXHLKtfweF',\n",
    " 'length': '50',\n",
    " 'permalink': 'https://theysaidso.com/quote/dPKsui4sQnQqgMnXHLKtfweF/rumi-where-there-is-ruin-there-is-hope-for-a-treasure',\n",
    " 'quote': 'Where there is ruin, there is hope for a treasure.',\n",
    " 'tags': ['failure', 'inspire', 'learning-from-failure'],\n",
    " 'title': 'Inspiring Quote of the day'}\n",
    "\n",
    "#extract only quote\n",
    "print(q['quote'], '\\n--', q['author'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 1-5. Collecting Data from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install bs4\n",
    "import urllib.request as urllib2 \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "response = urllib2.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "html_doc = response.read()\n",
    "\n",
    "#Parsing\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "# Formating the parsed html file\n",
    "strhtm = soup.prettify()\n",
    "\n",
    "# Print few lines\n",
    "print (strhtm[:1000])\n",
    "\n",
    "print(soup.title)\n",
    "print(soup.title.string)\n",
    "print(soup.a.string)\n",
    "print(soup.b.string)\n",
    "\n",
    "for x in soup.find_all('a'): print(x.string)\n",
    "    \n",
    "for x in soup.find_all('p'): print(x.text)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 1-6. Parsing Text using Regular Expressions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6.A Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import library\n",
    "\n",
    "import re\n",
    "\n",
    "#run the split query\n",
    "\n",
    "re.split('\\s+','I like this book.')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6.B Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = \"For more details please mail us at: xyz@abc.com, pqr@mno.com\"\n",
    "\n",
    "addresses = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', doc)\n",
    "for address in addresses: \n",
    "    print(address)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6.C Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc = \"For more details please mail us at xyz@abc.com\"\n",
    "\n",
    "new_email_address = re.sub(r'([\\w\\.-]+)@([\\w\\.-]+)', r'pqr@mno.com', doc)\n",
    "print(new_email_address)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6.D Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import library\n",
    "\n",
    "import re\n",
    "import requests\n",
    "\n",
    "#url you want to extract\n",
    "url = 'https://www.gutenberg.org/files/2638/2638-0.txt'\n",
    "\n",
    "#function to extract\n",
    "def get_book(url):\n",
    " # Sends a http request to get the text from project Gutenberg\n",
    " raw = requests.get(url).text\n",
    " # Discards the metadata from the beginning of the book\n",
    " start = re.search(r\"\\*\\*\\* START OF THIS PROJECT GUTENBERG EBOOK .* \\*\\*\\*\",raw ).end()\n",
    " # Discards the metadata from the end of the book\n",
    " stop = re.search(r\"II\", raw).start()\n",
    " # Keeps the relevant text\n",
    " text = raw[start:stop]\n",
    " return text\n",
    "\n",
    "\n",
    "# processing\n",
    "def preprocess(sentence): \n",
    " return re.sub('[^A-Za-z0-9.]+' , ' ', sentence).lower()\n",
    "\n",
    "\n",
    "#calling the above function\n",
    "\n",
    "book = get_book(url)\n",
    "processed_book = preprocess(book)\n",
    "print(processed_book)\n",
    "\n",
    "# Count number of times \"the\" is appeared in the book\n",
    "len(re.findall(r'the', processed_book))\n",
    "\n",
    "#Replace \"i\" with \"I\"\n",
    "processed_book = re.sub(r'\\si\\s', \" I \", processed_book)\n",
    "print(processed_book)\n",
    "\n",
    "#find all occurance of text in the format \"abc--xyz\"\n",
    "re.findall(r'[a-zA-Z0-9]*--[a-zA-Z0-9]*', book)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 1-7. Handling Strings"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7.A Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "String_v1 = \"I am exploring NLP\"\n",
    "\n",
    "#To extract particular character or range of characters from string\n",
    "\n",
    "print(String_v1[0])\n",
    "\n",
    "#output\n",
    "“I”\n",
    "\n",
    "#To extract exploring\n",
    "\n",
    "print(String_v1[5:14])\n",
    "\n",
    "String_v2 = String_v1.replace(\"exploring\", \"learning\")\n",
    "print(String_v2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7.B Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = \"nlp\"\n",
    "s2 = \"machine learning\"\n",
    "s3 = s1+s2\n",
    "print(s3)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7.C Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use find function to fetch the starting index value of the sub string in whole string.\n",
    "\n",
    "var=\"I am learning NLP\"\n",
    "f= \"learn\"\n",
    "var.find(f)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Recipe 1-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Retrying (Retry(total=4, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x000002A50E30F518>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',)': /simple/bs4/\n",
      "  Retrying (Retry(total=3, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x000002A50E424128>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',)': /simple/bs4/\n",
      "  Retrying (Retry(total=2, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x000002A50E4242B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',)': /simple/bs4/\n",
      "  Retrying (Retry(total=1, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x000002A50E424F98>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',)': /simple/bs4/\n",
      "  Retrying (Retry(total=0, connect=None, read=None, redirect=None)) after connection broken by 'NewConnectionError('<pip._vendor.requests.packages.urllib3.connection.VerifiedHTTPSConnection object at 0x000002A50E424080>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed',)': /simple/bs4/\n",
      "  Could not find a version that satisfies the requirement bs4 (from versions: )\n",
      "No matching distribution found for bs4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): requests in c:\\users\\adarsha.shivananda\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from ipywidgets import FloatProgress\n",
    "from time import sleep\n",
    "from IPython.display import display\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "url = 'http://www.imdb.com/chart/top?ref_=nv_mv_250_6'\n",
    "\n",
    "result = requests.get(url)\n",
    "c = result.content\n",
    "soup = BeautifulSoup(c,\"lxml\")\n",
    "\n",
    "summary = soup.find('div',{'class':'article'})\n",
    "\n",
    "# Create empty lists to append the extracted data.\n",
    "\n",
    "moviename = []\n",
    "cast = []\n",
    "description = []\n",
    "rating = []\n",
    "ratingoutof = []\n",
    "year = []\n",
    "genre = []\n",
    "movielength = []\n",
    "rot_audscore = []\n",
    "rot_avgrating = []\n",
    "rot_users = []\n",
    "\n",
    "# Extracting the required data from the html soup.\n",
    "\n",
    "rgx = re.compile('[%s]' % '()')\n",
    "f = FloatProgress(min=0, max=250)\n",
    "display(f)\n",
    "for row,i in zip(summary.find('table').findAll('tr'),range(len(summary.find('table').findAll('tr')))):\n",
    "    for sitem in row.findAll('span',{'class':'secondaryInfo'}):\n",
    "        s = sitem.find(text=True)\n",
    "        year.append(rgx.sub('', s))\n",
    "    for ritem in row.findAll('td',{'class':'ratingColumn imdbRating'}):\n",
    "        for iget in ritem.findAll('strong'):\n",
    "            rating.append(iget.find(text=True))\n",
    "            ratingoutof.append(iget.get('title').split(' ', 4)[3])\n",
    "    for item in row.findAll('td',{'class':'titleColumn'}):\n",
    "        for href in item.findAll('a',href=True):\n",
    "            moviename.append(href.find(text=True))\n",
    "            rurl = 'https://www.rottentomatoes.com/m/'+ href.find(text=True)\n",
    "            try:\n",
    "                rresult = requests.get(rurl)\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                status_code = \"Connection refused\"\n",
    "            rc = rresult.content\n",
    "            rsoup = BeautifulSoup(rc)\n",
    "            try:\n",
    "                rot_audscore.append(rsoup.find('div',{'class':'meter-value'}).find('span',{'class':'superPageFontColor'}).text)\n",
    "                rot_avgrating.append(rsoup.find('div',{'class':'audience-info hidden-xs superPageFontColor'}).find('div').contents[2].strip())\n",
    "                rot_users.append(rsoup.find('div',{'class':'audience-info hidden-xs superPageFontColor'}).contents[3].contents[2].strip())\n",
    "            except AttributeError:\n",
    "                rot_audscore.append(\"\")\n",
    "                rot_avgrating.append(\"\")\n",
    "                rot_users.append(\"\")\n",
    "            cast.append(href.get('title'))\n",
    "            imdb = \"http://www.imdb.com\" + href.get('href')\n",
    "            try:\n",
    "                iresult = requests.get(imdb)\n",
    "                ic = iresult.content\n",
    "                isoup = BeautifulSoup(ic)\n",
    "                description.append(isoup.find('div',{'class':'summary_text'}).find(text=True).strip())\n",
    "                genre.append(isoup.find('span',{'class':'itemprop'}).find(text=True))\n",
    "                movielength.append(isoup.find('time',{'itemprop':'duration'}).find(text=True).strip())\n",
    "            except requests.exceptions.ConnectionError:\n",
    "                description.append(\"\")\n",
    "                genre.append(\"\")\n",
    "                movielength.append(\"\")\n",
    "    sleep(.1)\n",
    "    f.value = i\n",
    "\n",
    "    # List to pandas series\n",
    "\n",
    "moviename = Series(moviename)\n",
    "cast = Series(cast)\n",
    "description = Series(description)\n",
    "rating = Series(rating)\n",
    "ratingoutof = Series(ratingoutof)\n",
    "year = Series(year)\n",
    "genre = Series(genre)\n",
    "movielength = Series(movielength) \n",
    "rot_audscore = Series(rot_audscore)\n",
    "rot_avgrating = Series(rot_avgrating)\n",
    "rot_users = Series(rot_users)\n",
    "\n",
    "# creating dataframe and doing analysis\n",
    "\n",
    "imdb_df = pd.concat([moviename,year,description,genre,movielength,cast,rating,ratingoutof,rot_audscore,rot_avgrating,rot_users],axis=1)\n",
    "imdb_df.columns = ['moviename','year','description','genre','movielength','cast','imdb_rating','imdb_ratingbasedon','tomatoes_audscore','tomatoes_rating','tomatoes_ratingbasedon']\n",
    "imdb_df['rank'] = imdb_df.index + 1\n",
    "imdb_df.head(1)\n",
    "\n",
    "# Saving the file as CSV.\n",
    "\n",
    "imdb_df.to_csv(\"imdbdataexport.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
